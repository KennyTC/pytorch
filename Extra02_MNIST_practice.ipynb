{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extra02-MNIST practice.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPxMy5UOqGwWU1PLmSoUSOZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KennyThinh/pytorch/blob/main/Extra02_MNIST_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHmBfdaLRO_4"
      },
      "source": [
        "Calculate loss function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sX-ATJdJRTTj"
      },
      "source": [
        "# Initialize the scores and ground truth\n",
        "logits = torch.tensor([[-1.2, 0.12, 4.8]])\n",
        "ground_truth = torch.tensor([2])\n",
        "\n",
        "# Instantiate cross entropy loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Compute and print the loss\n",
        "loss = criterion(logits, ground_truth)\n",
        "print(loss)\n",
        "\n",
        "# first calculate softmax --> probabilty --> -ln( probability of the index which is the ground truth) --> 0.0117"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWgr3h5qUGwS"
      },
      "source": [
        "MNIST full example\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKaGFF-eNtlG"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # Declare all the layers for feature extraction\n",
        "        self.features = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1), \n",
        "                                      nn.ReLU(inplace=True), nn.BatchNorm2d(num_feature=5),\n",
        "                                      nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1), \n",
        "                                      nn.MaxPool2d(2, 2), nn.ReLU(inplace=True), nn.BatchNorm2d(num_feature=10),\n",
        "                                      nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, padding=1),\n",
        "                                      nn.ReLU(inplace=True),nn.BatchNorm2d(num_feature=20),\n",
        "                                      nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3, padding=1),\n",
        "                                      nn.MaxPool2d(2, 2), nn.ReLU(inplace=True))\n",
        "        \n",
        "        # Declare all the layers for classification\n",
        "        # dropout should be used only for fully connected\n",
        "        self.classifier = nn.Sequential(nn.Linear(40 * 7 * 7, 1024), nn.ReLU(inplace=True), nn.Dropout(p=0.5),\n",
        "                                       \tnn.Linear(1024, 2048), nn.ReLU(inplace=True),nn.Dropout(p=0.5),\n",
        "                                        nn.Linear(2048, 10))\n",
        "    def forward(self, x):\n",
        "  \n",
        "      # Apply the feature extractor in the input\n",
        "      x = self.features(x)\n",
        "      \n",
        "      # Squeeze the three spatial dimensions in one\n",
        "      x = x.view(-1, 40 * 7 * 7)\n",
        "      \n",
        "      # Classify the images\n",
        "      x = self.classifier(x)\n",
        "      return x\n",
        "\n",
        "# Transform the data to torch tensors and normalize it \n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307), ((0.3081)))])\n",
        "\n",
        "# Prepare training set and testing set\n",
        "trainset = torchvision.datasets.MNIST('mnist', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.MNIST('mnist', train=False, download=True, transform=transform)\n",
        "\n",
        "\n",
        "\n",
        "# Shuffle the indices\n",
        "indices = np.arange(60000)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "# Build the train loader\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=False, sampler=torch.utils.data.SubsetRandomSampler(indices[:55000])) #SubsetRandomSampler and Shuffle=True are mutual exclusive\n",
        "\n",
        "# Build the validation loader\n",
        "val_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=False, sampler=torch.utils.data.SubsetRandomSampler(indices[55000:]))\n",
        "\n",
        "# Build the test loader\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=0) \n",
        "\n",
        "\n",
        "# Compute the shape of the training set and testing set\n",
        "trainset_shape = trainloader.dataset.train_data.shape\n",
        "testset_shape = testloader.dataset.test_data.shape\n",
        "\n",
        "# Print the computed shapes\n",
        "print(trainset_shape, testset_shape)\n",
        "\n",
        "\n",
        "# Instantiate the network\n",
        "model = Net()\n",
        "\n",
        "# Instantiate the cross-entropy loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Instantiate the Adam optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=0.001)\n",
        "\n",
        "\n",
        "for i, data in enumerate(train_loader, 0):\n",
        "    inputs, labels = data\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Compute the forward pass\n",
        "    outputs = net(inputs)\n",
        "        \n",
        "    # Compute the loss function\n",
        "    loss = criterion(outputs, labels)\n",
        "        \n",
        "    # Compute the gradients\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update the weights\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdtVO1hPP3X8"
      },
      "source": [
        "# Set the model in eval mode\n",
        "model.eval()\n",
        "\n",
        "for i, data in enumerate(test_loader, 0):\n",
        "    inputs, labels = data\n",
        "    \n",
        "    # Put each image into a vector\n",
        "    inputs = inputs.view(-1, 28*28)\n",
        "    \n",
        "    # Do the forward pass and get the predictions\n",
        "    outputs = model(inputs)\n",
        "    _, outputs = torch.max(outputs.data, 1) # 1 is to take argmax, 0 for max\n",
        "    total += labels.size(0)\n",
        "    correct += (outputs == labels).sum().item()\n",
        "print('The testing set accuracy of the network is: %d %%' % (100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX5A_732P4B2"
      },
      "source": [
        "Transfer learning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MC_v0RGP8Vi"
      },
      "source": [
        "# Import the module\n",
        "import torchvision\n",
        "\n",
        "# Download resnet18\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "# Freeze all the layers bar the last one\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Change the number of output units\n",
        "model.fc = nn.Linear(512, 7)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVDCV0azQC1U"
      },
      "source": [
        "# Create a model using\n",
        "model = Net()\n",
        "\n",
        "# Load the parameters from the old model\n",
        "model.load_state_dict(torch.load('my_net.pth'))\n",
        "\n",
        "# Change the number of out channels\n",
        "model.fc = nn.Linear(7 * 7 * 512, 26)\n",
        "\n",
        "# Train and evaluate the model\n",
        "model.train()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}